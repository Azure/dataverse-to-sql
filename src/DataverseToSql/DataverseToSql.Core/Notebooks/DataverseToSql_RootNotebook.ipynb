{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        },
        "description": null,
        "save_output": true,
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 23,
            "outputs": [],
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "tags": [
                    "parameters"
                ]
            },
            "source": [
                "target_schema = '<target schema>'\r\n",
                "storage_account = '<storage account>'\r\n",
                "container = '<container>'\r\n",
                "servername = '<server>'\r\n",
                "dbname = '<database>'\r\n",
                "entity_concurrency = 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "outputs": [],
            "metadata": {
                "tags": []
            },
            "source": [
                "import pyodbc\n",
                "import struct\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "\n",
                "sql_token = mssparkutils.credentials.getToken('DW')\n",
                "token_bytes = sql_token.encode(\"UTF-16-LE\")\n",
                "token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
                "\n",
                "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
                "conn = pyodbc.connect( 'DRIVER={ODBC Driver 18 for SQL Server};'\n",
                "                       f'SERVER={servername};'\n",
                "                       f'DATABASE={dbname}', \\\n",
                "                       attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
                "\n",
                "cursor = conn.execute(  'SELECT DISTINCT [EntityName]'\n",
                "                        'FROM [DataverseToSql].[BlobsToIngest]'\n",
                "                        'WHERE [LoadType] IN (0, 2) AND [Complete] = 0')\n",
                "\n",
                "def run_notebook(entity):\n",
                "    mssparkutils.notebook.run('DataverseToSql_ProcessEntity', 86400, {\n",
                "        \"entity\": entity,\n",
                "        \"target_schema\": target_schema,\n",
                "        \"storage_account\": storage_account,\n",
                "        \"container\": container,\n",
                "        \"servername\": servername,\n",
                "        \"dbname\": dbname,\n",
                "        })\n",
                "\n",
                "with ThreadPoolExecutor(max_workers = entity_concurrency) as executor:\n",
                "    for result in executor.map(run_notebook, [row[0] for row in cursor]):\n",
                "        print(result)"
            ]
        }
    ]
}
