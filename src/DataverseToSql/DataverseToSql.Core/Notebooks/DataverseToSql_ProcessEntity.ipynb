{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "entity = '<entity>'                     # Name of the entity to process\r\n",
                "target_schema = '<target schema>'       # Schema of the target table\r\n",
                "\r\n",
                "storage_account = '<storage account>'   # Storage account name (no FQDN)\r\n",
                "container = '<container>'               # Container for Synapse Link for Dataverse\r\n",
                "servername = '<server>'                 # Azure SQL Database server name\r\n",
                "dbname = '<database>'                   # Azure SQL Database name"
            ],
            "outputs": [],
            "execution_count": 1,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "tags": [
                    "parameters"
                ]
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.functions import to_timestamp, col, dense_rank, desc, rank,row_number, coalesce\r\n",
                "from pyspark.sql.window import Window\r\n",
                "import json\r\n",
                "import pyspark.sql.types as types\r\n",
                "import pyodbc\r\n",
                "import re\r\n",
                "import struct"
            ],
            "outputs": [],
            "execution_count": 2,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Functions\r\n",
                "\r\n",
                "# Returns the spark datatype based on the CDM model datatype\r\n",
                "def get_attribute_spark_datatype(attribute):\r\n",
                "    match attribute['dataType']:\r\n",
                "        case 'boolean':\r\n",
                "            return types.BooleanType()\r\n",
                "        case 'dateTime':\r\n",
                "            return types.StringType()\r\n",
                "        case 'decimal':\r\n",
                "            numeric_trait = [t for t in attribute[\"cdm:traits\"] if t[\"traitReference\"] == \"is.dataFormat.numeric.shaped\"][0]\r\n",
                "            precision = int([a for a in numeric_trait[\"arguments\"] if a[\"name\"]==\"precision\"][0][\"value\"])\r\n",
                "            scale = int([a for a in numeric_trait[\"arguments\"] if a[\"name\"]==\"scale\"][0][\"value\"])\r\n",
                "            return types.DecimalType(precision, scale)\r\n",
                "        case 'double':\r\n",
                "            return types.DoubleType()\r\n",
                "        case 'guid':\r\n",
                "            return types.StringType()\r\n",
                "        case 'int64':\r\n",
                "            return types.LongType()\r\n",
                "        case 'string':\r\n",
                "            return types.StringType()\r\n",
                "        case _:\r\n",
                "            raise Exception(f\"Unsupported CDM data type: {attribute['dataType']}\")\r\n",
                "\r\n",
                "# Read the content of a file\r\n",
                "def get_file_content(path):\r\n",
                "    for file in mssparkutils.fs.ls(model_path):\r\n",
                "        pass \r\n",
                "\r\n",
                "    if not file.isFile:\r\n",
                "        raise ValueError(f\"The specified path is not a file: {path}\")\r\n",
                "\r\n",
                "    return mssparkutils.fs.head(model_path, file.size)\r\n",
                "\r\n",
                "\r\n",
                "# Get the AAD token to access Azure SQL Database\r\n",
                "def get_sql_token():\r\n",
                "    return mssparkutils.credentials.getToken('DW')\r\n",
                "\r\n",
                "# Open a new pyodbc SQL Server connection \r\n",
                "def get_pyodbc_sql_conn(server, database):\r\n",
                "    sql_token = get_sql_token()\r\n",
                "    token_bytes = sql_token.encode(\"UTF-16-LE\")\r\n",
                "    token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\r\n",
                "\r\n",
                "    SQL_COPT_SS_ACCESS_TOKEN = 1256\r\n",
                "    conn = pyodbc.connect( 'DRIVER={ODBC Driver 18 for SQL Server};'\r\n",
                "                        f'SERVER={server};'\r\n",
                "                        f'DATABASE={database}', \\\r\n",
                "                        attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\r\n",
                "                    \r\n",
                "    return conn\r\n",
                "\r\n",
                "re_blob_path = re.compile(r'https://([^/]+).blob.core.windows.net/([^/]+)/(.+)')\r\n",
                "\r\n",
                "# Get the list of partitions to process for the specified entity\r\n",
                "def get_entity_partitions_to_process(entity, server, database):\r\n",
                "    conn = get_pyodbc_sql_conn(server, database)\r\n",
                "    cursor = conn.execute(\r\n",
                "        'SELECT DISTINCT [Id], [BasePath], [Timestamp], [Partition]'\r\n",
                "        'FROM [DataverseToSql].[BlobsToIngest]'\r\n",
                "        'WHERE [EntityName] = ? AND [LoadType] = 0 AND [Complete] = 0',\r\n",
                "        entity)\r\n",
                "\r\n",
                "    ret = list(\r\n",
                "        (\r\n",
                "            jobid,\r\n",
                "            f'abfss://{path.group(2)}@{path.group(1)}.dfs.core.windows.net/{path.group(3)}/{timestamp}/{partition}_*{timestamp}.csv',\r\n",
                "            f'abfss://{path.group(2)}@{path.group(1)}.dfs.core.windows.net/{path.group(3)}/{timestamp}/{partition}_{timestamp}.parquet',\r\n",
                "        )\r\n",
                "        for jobid, path, timestamp, partition in ((row[0], re_blob_path.match(row[1]), row[2], row[3]) for row in cursor))\r\n",
                "\r\n",
                "    conn.close()\r\n",
                "\r\n",
                "    return ret\r\n",
                "\r\n",
                "# Get the list of partitions to load for the specified entity\r\n",
                "def get_entity_partitions_to_load(entity, server, database):\r\n",
                "    conn = get_pyodbc_sql_conn(server, database)\r\n",
                "    cursor = conn.execute(\r\n",
                "        'SELECT DISTINCT [Id], [BlobName]'\r\n",
                "        'FROM [DataverseToSql].[BlobsToIngest]'\r\n",
                "        'WHERE [EntityName] = ? AND [LoadType] = 2 AND [Complete] = 0',\r\n",
                "        entity)\r\n",
                "\r\n",
                "    ret = list(cursor)\r\n",
                "\r\n",
                "    conn.close()\r\n",
                "\r\n",
                "    return ret\r\n",
                "\r\n",
                "# Mark an injestion job as complete\r\n",
                "# For initial ingestion the job corresponds to an entity partition\r\n",
                "def mark_job_complete(jobid, server, database):\r\n",
                "    conn = get_pyodbc_sql_conn(server, database)\r\n",
                "    conn.execute('exec [DataverseToSql].[IngestionJobs_Complete] ?', jobid)\r\n",
                "    conn.commit()\r\n",
                "    conn.close()\r\n",
                "\r\n",
                "# Insert a parquet file in metadata for loading\r\n",
                "def insert_parquet_for_loading(entity, path, server, database):\r\n",
                "    conn = get_pyodbc_sql_conn(server, database)\r\n",
                "    conn.execute(   'INSERT [DataverseToSql].[BlobsToIngest]('\r\n",
                "                        'EntityName,'\r\n",
                "                        'BlobName,'\r\n",
                "                        'BasePath,'\r\n",
                "                        'Timestamp,'\r\n",
                "                        'Partition,'\r\n",
                "                        'LoadType,'\r\n",
                "                        'Complete'\r\n",
                "                    ') VALUES(?,?,?,?,?,?,?)', \r\n",
                "                    entity, path, \"\",\"\", \"\", 2, 0)\r\n",
                "    conn.commit()\r\n",
                "    conn.close()\r\n",
                "\r\n",
                "\r\n",
                "# Process a partition of an entity\r\n",
                "def process_partition(entity, source_path, target_path, schema, timestamp_cols, target_columns, server, database):\r\n",
                "    df_source = spark.read \\\r\n",
                "        .schema(schema) \\\r\n",
                "        .option(\"mode\", \"PERMISSIVE\") \\\r\n",
                "        .option(\"multiline\", True) \\\r\n",
                "        .option(\"header\", False) \\\r\n",
                "        .option(\"escape\", '\"') \\\r\n",
                "        .csv(source_path) \\\r\n",
                "        .repartition(1)\r\n",
                "\r\n",
                "    # Identify the columns that are common to the source file and the target table.\r\n",
                "    common_columns = [col for col in target_columns if col in df_source.columns]\r\n",
                "\r\n",
                "    # Convert datetime columns\r\n",
                "    for colname, formats in timestamp_cols:\r\n",
                "        df_source = df_source.withColumn(colname, coalesce(*[to_timestamp(col(colname), fmt) for fmt in formats]))\r\n",
                "\r\n",
                "    # Deduplicate the records and write to the target table\r\n",
                "    # Consider only the latest record for each Id\r\n",
                "    # The latest record is identified based on the fields SinkModifiedOn and versionnumber\r\n",
                "    order_by_columns = [desc(c) for c in [\"SinkModifiedOn\", \"versionnumber\"]]\r\n",
                "    rownum_colname = \"dv2sqlrownum\"\r\n",
                "\r\n",
                "    # Deleted record are skipped\r\n",
                "    df = df_source \\\r\n",
                "        .withColumn(rownum_colname, row_number().over(Window.partitionBy(\"Id\").orderBy(order_by_columns))) \\\r\n",
                "        .where(col(rownum_colname) == 1) \\\r\n",
                "        .where((col('IsDelete').isNull()) | (col('IsDelete') == False)) \\\r\n",
                "        .select(common_columns) \\\r\n",
                "        .write \\\r\n",
                "        .mode(\"overwrite\") \\\r\n",
                "        .parquet(target_path)\r\n",
                "\r\n",
                "    # Insert parquet file for later loading\r\n",
                "    insert_parquet_for_loading(entity, target_path, server, database)\r\n",
                "\r\n",
                "def load_partition(source_path, jdbc_url, target_table):\r\n",
                "    spark \\\r\n",
                "        .read \\\r\n",
                "        .parquet(source_path) \\\r\n",
                "        .write \\\r\n",
                "        .mode(\"append\") \\\r\n",
                "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
                "        .option(\"url\", jdbc_url) \\\r\n",
                "        .option(\"dbtable\", target_table) \\\r\n",
                "        .option(\"accessToken\", get_sql_token()) \\\r\n",
                "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
                "        .option(\"schemaCheckEnabled\", False) \\\r\n",
                "        .option(\"tableLock\", True) \\\r\n",
                "        .save()\r\n",
                "\r\n",
                "def truncate_table(table, servername, database):\r\n",
                "    conn = get_pyodbc_sql_conn(servername, database)\r\n",
                "    conn.execute(f'TRUNCATE TABLE {table}')\r\n",
                "    conn.commit()\r\n",
                "    conn.close()\r\n"
            ],
            "outputs": [],
            "execution_count": 3,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "target_table = f\"[{target_schema}].[{entity}]\"\r\n",
                "jdbc_url = \"jdbc:sqlserver://\" + servername + \";\" + \"databaseName=\" + dbname + \";\"\r\n",
                "\r\n",
                "# Read entity metadata\r\n",
                "model_path = f'abfss://{container}@{storage_account}.dfs.core.windows.net/Microsoft.Athena.TrickleFeedService/{entity}-model.json'\r\n",
                "model_json = json.loads(get_file_content(model_path))\r\n",
                "\r\n",
                "# Read target table schema\r\n",
                "# Note: this step does not load data from the target table; it retrieves metadata only\r\n",
                "target_df = spark.read \\\r\n",
                "    .format(\"jdbc\") \\\r\n",
                "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
                "    .option(\"url\", jdbc_url) \\\r\n",
                "    .option(\"accessToken\", get_sql_token()) \\\r\n",
                "    .option(\"dbtable\", target_table) \\\r\n",
                "    .load()\r\n",
                "\r\n",
                "target_field_map = dict((field.name, field) for field in target_df.schema)\r\n",
                "\r\n",
                "# Build the CSV schema and identify timestamp columns\r\n",
                "schema = types.StructType()\r\n",
                "timestamp_cols = []\r\n",
                "timestamp_formats = [ \"M/d/y h:m:s a\", \"y-M-d'T'H:m:s.SSSSSSSXXXXX\", \"y-M-d'T'H:m:sX\"]\r\n",
                "\r\n",
                "for attribute in model_json['entities'][0]['attributes']:\r\n",
                "    # Align CSV data types to the target table to avoid conversion errors during bulk load\r\n",
                "    # This could happen with optionset fields when the int datatype override is enabled\r\n",
                "    # Does not apply to dateTime columns\r\n",
                "    if attribute['name'] in target_field_map and attribute['dataType'] != 'dateTime':\r\n",
                "        schema.add(\r\n",
                "            field = attribute['name'],\r\n",
                "            data_type = target_field_map[attribute['name']].dataType,\r\n",
                "            nullable = target_field_map[attribute['name']].nullable\r\n",
                "        )\r\n",
                "    else:\r\n",
                "        schema.add(\r\n",
                "            field = attribute['name'],\r\n",
                "            data_type = get_attribute_spark_datatype(attribute),\r\n",
                "            nullable = True\r\n",
                "        )\r\n",
                "\r\n",
                "    if attribute['dataType'] == 'dateTime':\r\n",
                "        timestamp_cols.append((attribute['name'], timestamp_formats))\r\n",
                "\r\n",
                "# Iterate over the table one partition at a time\r\n",
                "# Partitions are ingested serially because table lock is used during load\r\n",
                "for jobid, source_path, parquet_path in get_entity_partitions_to_process(entity, servername, dbname):\r\n",
                "    process_partition(entity, source_path, parquet_path, schema, timestamp_cols, target_df.columns, servername, dbname)\r\n",
                "    mark_job_complete(jobid, servername, dbname)\r\n",
                "\r\n",
                "truncate_table(target_table, servername, dbname)\r\n",
                "\r\n",
                "for jobid, parquet_path in get_entity_partitions_to_load(entity, servername, dbname):\r\n",
                "    load_partition(parquet_path, jdbc_url, target_table)\r\n",
                "    mark_job_complete(jobid, servername, dbname)\r\n"
            ],
            "outputs": [],
            "execution_count": 4,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        },
        "description": null,
        "save_output": true,
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
